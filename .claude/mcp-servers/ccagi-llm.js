#!/usr/bin/env node

/**
 * CCAGI LLM MCP Server
 *
 * ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆStarboseï¼‰ãŠã‚ˆã³OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™MCPã‚µãƒ¼ãƒãƒ¼
 *
 * æä¾›ãƒ„ãƒ¼ãƒ«:
 * - llm__chat - ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§LLMã‚’å‘¼ã³å‡ºã™
 * - llm__complete - ãƒ†ã‚­ã‚¹ãƒˆè£œå®Œã‚’å®Ÿè¡Œ
 * - llm__code_generate - ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸå‘¼ã³å‡ºã—
 * - llm__analyze - ã‚³ãƒ¼ãƒ‰è§£æžãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼
 * - llm__status - LLMæŽ¥ç¶šçŠ¶æ…‹ç¢ºèª
 *
 * ã‚µãƒãƒ¼ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:
 * - starbose: Starbose LLM (ãƒ­ãƒ¼ã‚«ãƒ«/OpenAIäº’æ›)
 * - openai: OpenAI API
 * - anthropic: Anthropic Claude API
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from '@modelcontextprotocol/sdk/types.js';
import { readFileSync, existsSync } from 'fs';
import { join } from 'path';

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
const DEFAULT_CONFIG = {
  starbose: {
    baseUrl: 'http://69.5.22.110/v1',
    model: 'openai/gpt-oss-120b',
    maxTokens: 4096,
    temperature: 0.7,
  },
  openai: {
    baseUrl: 'https://api.openai.com/v1',
    model: 'gpt-4',
    maxTokens: 4096,
    temperature: 0.7,
  },
  anthropic: {
    baseUrl: 'https://api.anthropic.com/v1',
    model: 'claude-sonnet-4-20250514',
    maxTokens: 4096,
    temperature: 0.7,
  },
};

// è¨­å®šèª­ã¿è¾¼ã¿
function loadConfig() {
  const cwd = process.cwd();
  const ccagiPath = join(cwd, '.ccagi.yml');

  let config = { ...DEFAULT_CONFIG };

  if (existsSync(ccagiPath)) {
    try {
      const content = readFileSync(ccagiPath, 'utf-8');
      // ç°¡æ˜“YAMLãƒ‘ãƒ¼ã‚¹ï¼ˆllmè¨­å®šéƒ¨åˆ†ã®ã¿ï¼‰
      const llmMatch = content.match(/llm:([\s\S]*?)(?=\n\w|$)/);
      if (llmMatch) {
        const baseUrlMatch = llmMatch[1].match(/base_url:\s*["']?([^"'\n]+)/);
        const modelMatch = llmMatch[1].match(/model:\s*["']?([^"'\n]+)/);
        if (baseUrlMatch) config.starbose.baseUrl = baseUrlMatch[1].trim();
        if (modelMatch) config.starbose.model = modelMatch[1].trim();
      }
    } catch (e) {
      // è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
    }
  }

  // ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ã
  if (process.env.STARBOSE_BASE_URL) {
    config.starbose.baseUrl = process.env.STARBOSE_BASE_URL;
  }
  if (process.env.STARBOSE_MODEL) {
    config.starbose.model = process.env.STARBOSE_MODEL;
  }
  if (process.env.OPENAI_API_KEY) {
    config.openai.apiKey = process.env.OPENAI_API_KEY;
  }
  if (process.env.ANTHROPIC_API_KEY) {
    config.anthropic.apiKey = process.env.ANTHROPIC_API_KEY;
  }

  return config;
}

/**
 * OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™
 */
async function callOpenAICompatible(baseUrl, messages, options = {}) {
  const endpoint = `${baseUrl}/chat/completions`;

  const body = {
    model: options.model || 'default',
    messages: messages,
    max_tokens: options.maxTokens || 4096,
    temperature: options.temperature || 0.7,
    stream: false,
  };

  if (options.systemPrompt) {
    body.messages = [
      { role: 'system', content: options.systemPrompt },
      ...messages,
    ];
  }

  const headers = {
    'Content-Type': 'application/json',
  };

  if (options.apiKey) {
    headers['Authorization'] = `Bearer ${options.apiKey}`;
  }

  const response = await fetch(endpoint, {
    method: 'POST',
    headers,
    body: JSON.stringify(body),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`LLM API Error: ${response.status} - ${errorText}`);
  }

  const data = await response.json();
  return data;
}

/**
 * ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸLLMå‘¼ã³å‡ºã—
 */
async function callLLM(provider, messages, options = {}) {
  const config = loadConfig();

  switch (provider) {
    case 'starbose': {
      const providerConfig = config.starbose;
      return await callOpenAICompatible(providerConfig.baseUrl, messages, {
        model: options.model || providerConfig.model,
        maxTokens: options.maxTokens || providerConfig.maxTokens,
        temperature: options.temperature || providerConfig.temperature,
        systemPrompt: options.systemPrompt,
      });
    }

    case 'openai': {
      const providerConfig = config.openai;
      if (!providerConfig.apiKey) {
        throw new Error('OPENAI_API_KEY is not set');
      }
      return await callOpenAICompatible(providerConfig.baseUrl, messages, {
        model: options.model || providerConfig.model,
        maxTokens: options.maxTokens || providerConfig.maxTokens,
        temperature: options.temperature || providerConfig.temperature,
        systemPrompt: options.systemPrompt,
        apiKey: providerConfig.apiKey,
      });
    }

    case 'anthropic': {
      // Anthropic APIã¯åˆ¥å½¢å¼ã ãŒã€ã“ã“ã§ã¯æœªå®Ÿè£…ï¼ˆClaude Codeã‹ã‚‰ç›´æŽ¥ä½¿ç”¨å¯èƒ½ï¼‰
      throw new Error('Anthropic provider is not supported via this tool. Use Claude Code directly.');
    }

    default:
      throw new Error(`Unknown provider: ${provider}`);
  }
}

/**
 * LLMæŽ¥ç¶šãƒ†ã‚¹ãƒˆ
 */
async function testConnection(provider) {
  const config = loadConfig();
  const providerConfig = config[provider];

  if (!providerConfig) {
    return { success: false, error: `Unknown provider: ${provider}` };
  }

  try {
    const startTime = Date.now();
    const response = await callLLM(provider, [
      { role: 'user', content: 'Hello, respond with just "OK"' },
    ], { maxTokens: 10 });

    const elapsed = Date.now() - startTime;
    const content = response.choices?.[0]?.message?.content || '';

    return {
      success: true,
      provider,
      baseUrl: providerConfig.baseUrl,
      model: providerConfig.model,
      responseTime: elapsed,
      testResponse: content.substring(0, 50),
    };
  } catch (error) {
    return {
      success: false,
      provider,
      baseUrl: providerConfig.baseUrl,
      error: error.message,
    };
  }
}

// MCPã‚µãƒ¼ãƒãƒ¼è¨­å®š
const server = new Server(
  {
    name: 'ccagi-llm',
    version: '1.0.0',
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// ãƒ„ãƒ¼ãƒ«å®šç¾©
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'llm__chat',
        description: 'ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆStarboseï¼‰ã«ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€è³ªå•å¿œç­”ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãªã©ã«ä½¿ç”¨ã§ãã¾ã™ã€‚',
        inputSchema: {
          type: 'object',
          properties: {
            messages: {
              type: 'array',
              items: {
                type: 'object',
                properties: {
                  role: { type: 'string', enum: ['user', 'assistant', 'system'] },
                  content: { type: 'string' },
                },
                required: ['role', 'content'],
              },
              description: 'ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®é…åˆ—',
            },
            provider: {
              type: 'string',
              enum: ['starbose', 'openai'],
              description: 'LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: starboseï¼‰',
              default: 'starbose',
            },
            model: {
              type: 'string',
              description: 'ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰',
            },
            maxTokens: {
              type: 'number',
              description: 'æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°',
              default: 4096,
            },
            temperature: {
              type: 'number',
              description: 'æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ0.0-2.0ï¼‰',
              default: 0.7,
            },
          },
          required: ['messages'],
        },
      },
      {
        name: 'llm__complete',
        description: 'ãƒ†ã‚­ã‚¹ãƒˆè£œå®Œã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç¶šããƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚',
        inputSchema: {
          type: 'object',
          properties: {
            prompt: {
              type: 'string',
              description: 'è£œå®Œã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
            },
            provider: {
              type: 'string',
              enum: ['starbose', 'openai'],
              default: 'starbose',
            },
            maxTokens: {
              type: 'number',
              default: 2048,
            },
            temperature: {
              type: 'number',
              default: 0.7,
            },
          },
          required: ['prompt'],
        },
      },
      {
        name: 'llm__code_generate',
        description: 'ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸLLMå‘¼ã³å‡ºã—ã€‚è¨€èªžã€èª¬æ˜Žã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚',
        inputSchema: {
          type: 'object',
          properties: {
            description: {
              type: 'string',
              description: 'ç”Ÿæˆã—ãŸã„ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜Ž',
            },
            language: {
              type: 'string',
              description: 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªžï¼ˆtypescript, python, go, etc.ï¼‰',
              default: 'typescript',
            },
            context: {
              type: 'string',
              description: 'è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã€è¦ä»¶ãªã©ï¼‰',
            },
            provider: {
              type: 'string',
              enum: ['starbose', 'openai'],
              default: 'starbose',
            },
            maxTokens: {
              type: 'number',
              default: 4096,
            },
          },
          required: ['description'],
        },
      },
      {
        name: 'llm__analyze',
        description: 'ã‚³ãƒ¼ãƒ‰è§£æžãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã®å•é¡Œç‚¹ã€æ”¹å–„ææ¡ˆã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã¾ã™ã€‚',
        inputSchema: {
          type: 'object',
          properties: {
            code: {
              type: 'string',
              description: 'è§£æžå¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰',
            },
            analysisType: {
              type: 'string',
              enum: ['review', 'security', 'performance', 'refactor'],
              description: 'è§£æžã‚¿ã‚¤ãƒ—',
              default: 'review',
            },
            language: {
              type: 'string',
              description: 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªž',
            },
            provider: {
              type: 'string',
              enum: ['starbose', 'openai'],
              default: 'starbose',
            },
          },
          required: ['code'],
        },
      },
      {
        name: 'llm__status',
        description: 'LLMæŽ¥ç¶šçŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚Starbose LLMã®ç–Žé€šç¢ºèªã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’æ¸¬å®šã—ã¾ã™ã€‚',
        inputSchema: {
          type: 'object',
          properties: {
            provider: {
              type: 'string',
              enum: ['starbose', 'openai', 'all'],
              default: 'starbose',
            },
          },
        },
      },
    ],
  };
});

// ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒãƒ³ãƒ‰ãƒ©ãƒ¼
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;

  try {
    switch (name) {
      case 'llm__chat': {
        const { messages, provider = 'starbose', model, maxTokens, temperature } = args;

        const response = await callLLM(provider, messages, {
          model,
          maxTokens,
          temperature,
        });

        const content = response.choices?.[0]?.message?.content || '';
        const usage = response.usage || {};

        let resultText = `ðŸ¤– LLM Response (${provider})\n\n`;
        resultText += content;
        resultText += `\n\n---\n`;
        resultText += `Model: ${response.model || 'unknown'}\n`;
        resultText += `Tokens: ${usage.total_tokens || 'N/A'} (prompt: ${usage.prompt_tokens || 'N/A'}, completion: ${usage.completion_tokens || 'N/A'})`;

        return {
          content: [{ type: 'text', text: resultText }],
        };
      }

      case 'llm__complete': {
        const { prompt, provider = 'starbose', maxTokens, temperature } = args;

        const messages = [{ role: 'user', content: prompt }];
        const response = await callLLM(provider, messages, { maxTokens, temperature });

        const content = response.choices?.[0]?.message?.content || '';

        return {
          content: [{ type: 'text', text: content }],
        };
      }

      case 'llm__code_generate': {
        const { description, language = 'typescript', context, provider = 'starbose', maxTokens } = args;

        const systemPrompt = `You are an expert ${language} programmer. Generate clean, well-documented, production-ready code. Only output the code without explanations unless asked.`;

        let userPrompt = `Generate ${language} code for the following:\n\n${description}`;
        if (context) {
          userPrompt += `\n\nContext:\n${context}`;
        }

        const messages = [{ role: 'user', content: userPrompt }];
        const response = await callLLM(provider, messages, {
          maxTokens,
          temperature: 0.3, // Lower temperature for code generation
          systemPrompt,
        });

        const content = response.choices?.[0]?.message?.content || '';

        let resultText = `ðŸ’» Generated Code (${language})\n\n`;
        resultText += content;

        return {
          content: [{ type: 'text', text: resultText }],
        };
      }

      case 'llm__analyze': {
        const { code, analysisType = 'review', language, provider = 'starbose' } = args;

        const analysisPrompts = {
          review: 'Review this code for issues, bugs, and improvements. Provide specific feedback.',
          security: 'Analyze this code for security vulnerabilities. Check for OWASP Top 10 issues.',
          performance: 'Analyze this code for performance issues. Suggest optimizations.',
          refactor: 'Suggest refactoring improvements for this code. Focus on readability and maintainability.',
        };

        const systemPrompt = `You are a senior code reviewer. Provide detailed, actionable feedback.`;
        const userPrompt = `${analysisPrompts[analysisType]}\n\n${language ? `Language: ${language}\n\n` : ''}Code:\n\`\`\`\n${code}\n\`\`\``;

        const messages = [{ role: 'user', content: userPrompt }];
        const response = await callLLM(provider, messages, {
          maxTokens: 4096,
          temperature: 0.3,
          systemPrompt,
        });

        const content = response.choices?.[0]?.message?.content || '';

        let resultText = `ðŸ“ Code Analysis (${analysisType})\n\n`;
        resultText += content;

        return {
          content: [{ type: 'text', text: resultText }],
        };
      }

      case 'llm__status': {
        const { provider = 'starbose' } = args;
        const config = loadConfig();

        let resultText = 'ðŸ”Œ LLM Connection Status\n\n';

        if (provider === 'all') {
          // Test all providers
          for (const p of ['starbose', 'openai']) {
            const result = await testConnection(p);
            resultText += `=== ${p.toUpperCase()} ===\n`;
            if (result.success) {
              resultText += `Status: âœ… Connected\n`;
              resultText += `URL: ${result.baseUrl}\n`;
              resultText += `Model: ${result.model}\n`;
              resultText += `Response Time: ${result.responseTime}ms\n`;
              resultText += `Test Response: "${result.testResponse}"\n\n`;
            } else {
              resultText += `Status: âŒ Failed\n`;
              resultText += `URL: ${result.baseUrl || 'N/A'}\n`;
              resultText += `Error: ${result.error}\n\n`;
            }
          }
        } else {
          const result = await testConnection(provider);
          if (result.success) {
            resultText += `Provider: ${result.provider}\n`;
            resultText += `Status: âœ… Connected\n`;
            resultText += `Base URL: ${result.baseUrl}\n`;
            resultText += `Model: ${result.model}\n`;
            resultText += `Response Time: ${result.responseTime}ms\n`;
            resultText += `Test Response: "${result.testResponse}"\n`;
          } else {
            resultText += `Provider: ${result.provider}\n`;
            resultText += `Status: âŒ Connection Failed\n`;
            resultText += `Base URL: ${result.baseUrl || 'N/A'}\n`;
            resultText += `Error: ${result.error}\n`;
          }
        }

        resultText += '\n=== Configuration ===\n';
        resultText += `Starbose URL: ${config.starbose.baseUrl}\n`;
        resultText += `Starbose Model: ${config.starbose.model}\n`;
        resultText += `OpenAI API Key: ${config.openai.apiKey ? 'âœ… Set' : 'âŒ Not set'}\n`;

        return {
          content: [{ type: 'text', text: resultText }],
        };
      }

      default:
        throw new Error(`Unknown tool: ${name}`);
    }
  } catch (error) {
    return {
      content: [{ type: 'text', text: `âŒ Error: ${error.message}` }],
      isError: true,
    };
  }
});

// ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
async function main() {
  const transport = new StdioServerTransport();
  await server.connect(transport);
  console.error('CCAGI LLM MCP Server running on stdio');
}

main().catch((error) => {
  console.error('Server error:', error);
  process.exit(1);
});
